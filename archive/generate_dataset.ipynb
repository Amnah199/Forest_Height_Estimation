{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest_height/blob/main/archive/generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "u73c0Q_dtc2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaXIeWlctulu",
        "outputId": "f9ce347f-609c-4716-f54a-cb548f8580a4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the zipped data is uploaded in the root_path folder\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'\n",
        "user = \"lgiesen\"\n",
        "repo = \"forest_height\"\n",
        "!git clone https://github.com/{user}/{repo}.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHOyEgCG4vju",
        "outputId": "550a2fb6-a673-44e9-ee60-07629612af9b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'forest_height'...\n",
            "remote: Enumerating objects: 563, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 563 (delta 114), reused 202 (delta 109), pack-reused 348\u001b[K\n",
            "Receiving objects: 100% (563/563), 38.49 MiB | 19.46 MiB/s, done.\n",
            "Resolving deltas: 100% (305/305), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/forest_height/src/generate_data.py"
      ],
      "metadata": {
        "id": "nNDKskC-hrBu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# unzip data\n",
        "%cd \"drive/MyDrive/Colab Notebooks/data/\"\n",
        "# use -B flag to rename files if there is a file with its name\n",
        "!for f in *.zip; do unzip -B \"$f\"; done\n",
        "%cd ../../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j6QFmDk78E9",
        "outputId": "b3971dd4-28c1-499a-ec7f-7da5f0f15323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data\n",
            "Archive:  images_02.zip\n",
            "  inflating: images/image_000.npy    \n",
            "  inflating: images/image_001.npy    \n",
            "  inflating: images/image_002.npy    \n",
            "  inflating: images/image_003.npy    \n",
            "  inflating: images/image_004.npy    \n",
            "  inflating: images/image_005.npy    \n",
            "  inflating: images/image_006.npy    \n",
            "  inflating: images/image_007.npy    \n",
            "  inflating: images/image_008.npy    \n",
            "  inflating: images/image_009.npy    \n",
            "  inflating: images/image_010.npy    \n",
            "  inflating: images/image_011.npy    \n",
            "  inflating: images/image_012.npy    \n",
            "  inflating: images/image_013.npy    \n",
            "  inflating: images/image_014.npy    \n",
            "  inflating: images/image_015.npy    \n",
            "  inflating: images/image_016.npy    \n",
            "  inflating: images/image_017.npy    \n",
            "  inflating: images/image_018.npy    \n",
            "  inflating: images/image_019.npy    \n",
            "Archive:  images_train.zip\n",
            "  inflating: images/image_000.npy    \n",
            "  inflating: images/image_001.npy    \n",
            "  inflating: images/image_002.npy    \n",
            "  inflating: images/image_003.npy    \n",
            "  inflating: images/image_004.npy    \n",
            "  inflating: images/image_005.npy    \n",
            "  inflating: images/image_006.npy    \n",
            "  inflating: images/image_007.npy    \n",
            "  inflating: images/image_008.npy    \n",
            "  inflating: images/image_009.npy    \n",
            "  inflating: images/image_010.npy    \n",
            "  inflating: images/image_011.npy    \n",
            "  inflating: images/image_012.npy    \n",
            "  inflating: images/image_013.npy    \n",
            "  inflating: images/image_014.npy    \n",
            "  inflating: images/image_015.npy    \n",
            "  inflating: images/image_016.npy    \n",
            "  inflating: images/image_017.npy    \n",
            "  inflating: images/image_018.npy    \n",
            "  inflating: images/image_019.npy    \n",
            "Archive:  masks_02.zip\n",
            "  inflating: masks/mask_000.npy      \n",
            "  inflating: masks/mask_001.npy      \n",
            "  inflating: masks/mask_002.npy      \n",
            "  inflating: masks/mask_003.npy      \n",
            "  inflating: masks/mask_004.npy      \n",
            "  inflating: masks/mask_005.npy      \n",
            "  inflating: masks/mask_006.npy      \n",
            "  inflating: masks/mask_007.npy      \n",
            "  inflating: masks/mask_008.npy      \n",
            "  inflating: masks/mask_009.npy      \n",
            "  inflating: masks/mask_010.npy      \n",
            "  inflating: masks/mask_011.npy      \n",
            "  inflating: masks/mask_012.npy      \n",
            "  inflating: masks/mask_013.npy      \n",
            "  inflating: masks/mask_014.npy      \n",
            "  inflating: masks/mask_015.npy      \n",
            "  inflating: masks/mask_016.npy      \n",
            "  inflating: masks/mask_017.npy      \n",
            "  inflating: masks/mask_018.npy      \n",
            "  inflating: masks/mask_019.npy      \n",
            "Archive:  masks_train.zip\n",
            "  inflating: masks/mask_000.npy      \n",
            "  inflating: masks/mask_001.npy      \n",
            "  inflating: masks/mask_002.npy      \n",
            "  inflating: masks/mask_003.npy      \n",
            "  inflating: masks/mask_004.npy      \n",
            "  inflating: masks/mask_005.npy      \n",
            "  inflating: masks/mask_006.npy      \n",
            "  inflating: masks/mask_007.npy      \n",
            "  inflating: masks/mask_008.npy      \n",
            "  inflating: masks/mask_009.npy      \n",
            "  inflating: masks/mask_010.npy      \n",
            "  inflating: masks/mask_011.npy      \n",
            "  inflating: masks/mask_012.npy      \n",
            "  inflating: masks/mask_013.npy      \n",
            "  inflating: masks/mask_014.npy      \n",
            "  inflating: masks/mask_015.npy      \n",
            "  inflating: masks/mask_016.npy      \n",
            "  inflating: masks/mask_017.npy      \n",
            "  inflating: masks/mask_018.npy      \n",
            "  inflating: masks/mask_019.npy      \n",
            "/content\n",
            "CPU times: user 361 ms, sys: 48.6 ms, total: 409 ms\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset(X_train, X_test, y_train, y_test):\n",
        "  X_train.to_pickle(\"X_train.pkl\")\n",
        "  y_train.to_pickle(\"y_train.pkl\")\n",
        "  X_test.to_pickle(\"X_test.pkl\")\n",
        "  y_test.to_pickle(\"y_test.pkl\")\n",
        "  %cd ../../"
      ],
      "metadata": {
        "id": "g-vRVuT5cJyS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make directories for all types of datasets."
      ],
      "metadata": {
        "id": "2dVgRqAYl1nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd forest_height/data/\n",
        "!mkdir color_channels color_channels_ndvi ndvi vi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uio4sCnJc-LR",
        "outputId": "ecb78398-4ef7-4419-8c64-590a4339a546"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/forest_height/data\n",
            "mkdir: cannot create directory ‘color_channels’: File exists\n",
            "mkdir: cannot create directory ‘color_channels_ndvi’: File exists\n",
            "mkdir: cannot create directory ‘ndvi’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'\n",
        "\n",
        "\n",
        "def get_files(dir):\n",
        "    \"\"\"\n",
        "    Get all files from a directory\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dir: Array of strings\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Array of strings\n",
        "    \"\"\"\n",
        "    return [f for f in listdir(dir) if isfile(join(dir, f))]\n",
        "\n",
        "def extract_data(path_images, path_masks):\n",
        "    \"\"\"\n",
        "    Extract data from zipped files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_images: String\n",
        "    path_masks: String\n",
        "    Path to the train data (default: None)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset: Tuple of np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    # load satellite images by loading the first one and then concatenating the rest\n",
        "    X = np.load(f'{path_images}{get_files(path_images)[0]}')\n",
        "    for filename in get_files(path_images)[1:]:\n",
        "        temp = np.load(f'{path_images}{filename}', allow_pickle=True)\n",
        "        X = np.concatenate((X, temp))\n",
        "    # reshape X to distinguish between image and color channel\n",
        "    num_imgs = len(get_files(path_images))\n",
        "    X = X.reshape((num_imgs, int(X.shape[0]/num_imgs), X.shape[1], X.shape[2]))\n",
        "    # ceil the values at 2000 because clouds have a different reflection value\n",
        "    ceiling = 2000\n",
        "    X[X > ceiling] = ceiling\n",
        "    #scale values between 0 and 1\n",
        "    X = X / ceiling\n",
        "\n",
        "    # load labels by loading the first one and then concatenating the rest\n",
        "    y = np.load(f'{path_masks}{get_files(path_masks)[0]}')\n",
        "    for filename in get_files(path_masks)[1:]:\n",
        "        temp = np.load(f'{path_masks}{filename}', allow_pickle=True)\n",
        "        y = np.concatenate((y, temp))\n",
        "\n",
        "    del temp, ceiling, num_imgs\n",
        "\n",
        "    return (X, y)\n",
        "\n",
        "def extract_labels(X, y):\n",
        "    \"\"\"\n",
        "    Labels are sparse, so they are Get all labels (non-zero elements) from a set of images\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: numpy.ndarray\n",
        "    y: numpy.ndarray\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df: pandas.DataFrame\n",
        "    \"\"\"\n",
        "    # extract non-zero value indices from y (= label position) to extract the corresponding X-value\n",
        "    # prepare data to merge it into one data frame,\n",
        "    # which makes it easier to extract the values of the same pixel\n",
        "    X = X.reshape(10, -1)\n",
        "    y = y.reshape(1, -1)\n",
        "    Xy = np.concatenate((X, y), axis=0)\n",
        "    Xy = Xy.transpose()\n",
        "    data = np.empty((0,11))\n",
        "    data = np.concatenate((data, Xy), axis=0)\n",
        "\n",
        "    indices = np.nonzero(data[:,-1])\n",
        "    labeled_data = data[indices]\n",
        "\n",
        "    # create data frame with features and labels\n",
        "    df = pd.DataFrame(labeled_data)\n",
        "    df.columns = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'Label']\n",
        "\n",
        "    return df\n",
        "\n",
        "def upsample_data(df):\n",
        "    \"\"\"\n",
        "    Upsample underrepresented data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pandas.DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features: pandas.DataFrame\n",
        "    labels: pandas.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # sort data according to tree height asc\n",
        "    dfs = df.sort_values('Label').reset_index(drop=True)\n",
        "    # create empty data frame to fill\n",
        "    dff = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    index_start = 0\n",
        "    for i in range(3, 37, 3):\n",
        "        #count the number of instances that are in one interval for example 0 - 3 or 15 - 18\n",
        "        index_end = index_start + dfs[\"Label\"][(dfs[\"Label\"] > i - 3) & (dfs[\"Label\"] < i)].count()\n",
        "        # take random sample of the interval\n",
        "        samp = dfs[index_start:index_end].sample(800)\n",
        "        dff = pd.concat((dff, samp))\n",
        "        index_start = index_end\n",
        "\n",
        "    # add the highest values because there are only a few\n",
        "    dff = pd.concat((dff, dfs[index_start:]))\n",
        "     #shuffle the dataset randomly\n",
        "    return dff.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "def calculate_ndvi(X):\n",
        "    \"\"\"\n",
        "    Generate a dataset (X_train, X_test, y_train, y_test) based on the location of zip files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: pd.DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    # Extract the relevant bands for NDVI calculation\n",
        "    b4, b8 = X['B4'], X['B8']\n",
        "    # Calculate NDVI\n",
        "    ndvi = (b8 - b4) / (b8 + b4)\n",
        "\n",
        "    # Add NDVI as a new feature to X\n",
        "    X[\"NDVI\"] = ndvi\n",
        "    return X\n",
        "\n",
        "def calculate_VIs(X):\n",
        "    \"\"\"\n",
        "    Generate a dataset (X_train, X_test, y_train, y_test) based on the location of zip files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: pd.DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    # Extract the relevant bands for calculating VIs\n",
        "    b2, b4, b5, b6 = X['B2'], X['B4'], X['B5'], X['B6']\n",
        "    b7, b8 = X['B7'], X['B8']\n",
        "\n",
        "    # Enhanced Vegetation Index\n",
        "    evi = 2.5 * ((b8 - b4) / (b8 + 6 * b4 - 7.5 * b2 + 1))\n",
        "\n",
        "    # Soil Adjusted Vegetation Index (SAVI)\n",
        "    savi = (b8 - b4) / (b8 + b4 + 0.428) * (1.428)\n",
        "\n",
        "    # Inverted Red-Edge Chlorophyll Index\n",
        "    ireci =  b7 - b4 / (b5 / b6)\n",
        "\n",
        "    # Sentinel 2 Red Edge Position\n",
        "    s2rep = 705 + 35 * ((b7 + b4) / 2 - b5) / (b6 - b5)\n",
        "\n",
        "    # Add VIs as a new feature to X\n",
        "    X[\"EVI\"] = evi\n",
        "    X[\"SAVI\"] = savi\n",
        "    X[\"IRECI\"] = ireci\n",
        "    X[\"s2rep\"] = s2rep\n",
        "    return X\n",
        "\n",
        "\n",
        "def generate_dataset(path_images, path_masks, output_variables, is_balanced = False):\n",
        "    \"\"\"\n",
        "    Generate a dataset (X_train, X_test, y_train, y_test) based on the location of zip files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_images: String\n",
        "    path_masks: String\n",
        "    output_variables: List (['color_channels', 'NDVI', 'VI'])\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    X, y = extract_data(path_images, path_masks)\n",
        "    Xy = extract_labels(X, y)\n",
        "    del X, y\n",
        "    if is_balanced:\n",
        "        Xy = upsample_data(Xy)\n",
        "    # extract features and labels\n",
        "    features = Xy.iloc[:, 0:10]\n",
        "    labels = Xy.iloc[:,10]\n",
        "\n",
        "    # check for each output variable\n",
        "    if 'NDVI' in output_variables:\n",
        "        features = calculate_ndvi(features)\n",
        "    if 'VI' in output_variables:\n",
        "        features = calculate_VIs(features)\n",
        "    if 'color_channels' not in output_variables:\n",
        "        features.drop(columns=features.columns[:10],axis=1, inplace=True)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0, shuffle=True)\n",
        "    del features, labels\n",
        "    return (X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "ryvG70tOIK1g"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only color channels"
      ],
      "metadata": {
        "id": "BvDYnMUOl7XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../..\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks, ['color_channels'])\n",
        "%cd forest_height/data/color_channels\n",
        "save_dataset(X_train, X_test, y_train, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "4K6bHgTBlrD-",
        "outputId": "67fc9d57-1755-4d7e-ef8a-f4cc1f7948c6"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-021edd15de8d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'color_channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forest_height/data/color_channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msave_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-21438e57cc00>\u001b[0m in \u001b[0;36mgenerate_dataset\u001b[0;34m(path_images, path_masks, output_variables, is_balanced)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \"\"\"\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mXy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-21438e57cc00>\u001b[0m in \u001b[0;36mextract_data\u001b[0;34m(path_images, path_masks)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# load satellite images by loading the first one and then concatenating the rest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path_images}{get_files(path_images)[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path_images}{filename}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-21438e57cc00>\u001b[0m in \u001b[0;36mget_files\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mArray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/data/images/'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Color channels and ndvi value"
      ],
      "metadata": {
        "id": "nR_XLa-JnHwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks, ['color_channels', 'NDVI'])\n",
        "%cd forest_height/data/color_channels_ndvi\n",
        "save_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "_97Kmf4qrHYA",
        "outputId": "0f00781e-b30e-433c-bae5-fc57d146c48f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/forest_height/data/color_channels_ndvi\n",
            "/content/forest_height\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only ndvi value"
      ],
      "metadata": {
        "id": "QUBnyCuAnJ-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks, ['NDVI'])\n",
        "%cd forest_height/data/ndvi\n",
        "save_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCo89CmuP4Y",
        "outputId": "a8849b76-7f9f-49e9-ff5b-b643200f675a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/forest_height/data/ndvi\n",
            "/content/forest_height\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks, ['NDVI', 'VI'])\n",
        "%cd forest_height/data/vi\n",
        "save_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "zW3r4x12JpMh",
        "outputId": "dde719cb-909d-4f99-e1f1-d2a4d1ba80b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "[Errno 2] No such file or directory: 'forest_height/data/vi'\n",
            "/content\n",
            "/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = extract_data(path_images, path_masks)\n",
        "df = extract_labels(X, y)\n",
        "del X, y\n",
        "df.columns"
      ],
      "metadata": {
        "id": "-SsyujTFKCLE",
        "outputId": "439c9822-e280-4948-86c5-799b16b02a38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'Label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # extract features and labels\n",
        "  features = dftr.iloc[:, 0:10]\n",
        "  labels = dftr.iloc[:,10]"
      ],
      "metadata": {
        "id": "vG4uiATZK7bn",
        "outputId": "7460f065-041c-477e-8cc6-a95afcc794a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidIndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '(slice(None, 10, None), 10)' is an invalid key",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-38fd48eae1e7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3807\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3808\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3807\u001b[0m                 \u001b[0;31m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3808\u001b[0m                 \u001b[0;31m#  the TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3809\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3810\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5923\u001b[0m             \u001b[0;31m# if key is not a scalar, directly raise an error (the code below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5924\u001b[0m             \u001b[0;31m# would convert to numpy arrays and raise later any way) - GH29926\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5925\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5927\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, 10, None), 10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove drive connection as it is no longer needed\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "wJ1a2sqeJhg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}