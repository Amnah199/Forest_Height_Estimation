{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest-height/blob/main/forest_height.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forest Height Estimation\n",
        "Objective of this notebook:\n",
        "- Train model\n",
        "- Evaluate model\n",
        "- Export model for prediction\n",
        "\n",
        "Prerequisite: \n",
        "- Training data is generated in [data_exploration.ipynb](https://github.com/lgiesen/forest-height/blob/main/data_exploration.ipynb)\n",
        "- Prediction of the dataset is realized in a python file (!to be created), but it is tested in a Jupyter Notebook (!to be created)"
      ],
      "metadata": {
        "id": "uwfy5ky_YNt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To Dos\n",
        "\n",
        "- Data Augmentation (flip image at least by 90, 180 and 270°)\n",
        "\n",
        "### Tasks from Presentation\n",
        "- Split the dataset into a training and validation set. Train a first regression model on the provided training dataset.\n",
        "- Try out different architectures, hyperparameter.\n",
        "- Now, you are supposed to apply your model to the test set. First, you have to\n",
        "implement the sliding window approach in combination with non-max suppression. Note: Instead of choosing the non-max suppression, you can choose a different\n",
        "approach or come up with your own.\n",
        "- After you have found your best performing setup, apply your model to the unlabeled data set. You can check for plausibility by visually inspecting the output or choose to reuse the predictions for increasing the number of training observations.\n",
        "\n",
        "- implement 1 ML Model (Logistic Regression, Boosted Trees, Random Forest, ...) and one CNN Model -> compare them in a poster (I'd use Figma as a tool)\n",
        "- submission: script that produces a binary output numpy-file (.npy) for every test image, automatically.\n",
        "\n",
        "\n",
        "### Submission\n",
        "- The npy-files should have the same height and width as the original satellite image. Your submission should therefore be a zip file containing multiple npy-files of size [1 × width × weight]\n",
        "- We will use the Mean Absolute Error (MAE) to measure your performance on the hidden test set. For this, we will provide further information at a later time.\n",
        "- Deadline: 4th July, 11:59pm\n",
        "\n",
        "- ZIP-file with all the predictions as described above\n",
        "- ZIP-file with your source code (only the files that are used in your final product)\n",
        "- A poster (A1-size) as pdf-file"
      ],
      "metadata": {
        "id": "uwfU-gIiYMDw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57beiuW6Nx-B",
        "outputId": "f01c5ced-93e5-4b3d-fb1b-7373e2bd0fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive', force_remount=True)\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cDh85SwKRKCe"
      },
      "outputs": [],
      "source": [
        "path_images = root_path + 'images/'\n",
        "path_masks = root_path + 'masks/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def path_exists(path):\n",
        "  import os\n",
        "  return os.path.exists(root_path + path)"
      ],
      "metadata": {
        "id": "m_Xk6fH-O6_t"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCgGU0-32oER",
        "outputId": "9c850db0-4dc9-41b9-d02b-417083c3ebed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "satellite: (10, 1024, 1024)\n",
            "mask: (10, 1024, 1024)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# load exemplary data\n",
        "sat_path = 'images/image_004.npy'\n",
        "if path_exists(sat_path):\n",
        "  satellite = np.load(root_path + sat_path)\n",
        "  print('satellite:',satellite.shape)\n",
        "mask_path = 'masks/mask_004.npy'\n",
        "if path_exists(mask_path):\n",
        "  mask = np.load(root_path + mask_path)\n",
        "  mask.shape\n",
        "  print('mask:',satellite.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cMQABaW6Q2pE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_img(img, is_satellite = True):\n",
        "    #shape: satellite == (10, 1024, 1024), mask == (1, 1024, 1024)\n",
        "    if is_satellite:\n",
        "      # Extract Red, Green, and Blue bands\n",
        "      red = img[2, :, :]\n",
        "      green = img[1, :, :]\n",
        "      blue = img[0, :, :]\n",
        "\n",
        "      # Normalize the bands to [0, 1] range\n",
        "      red_norm = (red - red.min()) / (red.max() - red.min())\n",
        "      green_norm = (green - green.min()) / (green.max() - green.min())\n",
        "      blue_norm = (blue - blue.min()) / (blue.max() - blue.min())\n",
        "    \n",
        "      # Stack the bands to create an RGB image\n",
        "      scaled_img = np.stack((red_norm, green_norm, blue_norm), axis=-1)\n",
        "    \n",
        "    elif not is_satellite:\n",
        "      scaled_img = (mask - np.min(mask)) / (np.amax(mask) - np.amin(mask))\n",
        "      # TODO: scale with total max and min of all masks for comparability\n",
        "      scaled_img = np.squeeze(scaled_img) # remove redundant dimension\n",
        "      \n",
        "      \n",
        "    # Plot the image\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(scaled_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1lFuJ_jWi5m"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtSyrAb3-pku"
      },
      "source": [
        "The npy files are combined into a dataset. After the first loading they do not have to be generated anymore.\n",
        "One npy file might need to be split into smaller images of 256x256 or 512x512 pixels, which then are put back together in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aZ_VFutfOgFP"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "path_train_sat = root_path + \"train_satellite.npy\"\n",
        "path_train_masks = root_path + \"train_masks.npy\"\n",
        "\n",
        "train_sat = np.load(path_train_sat, allow_pickle=True)\n",
        "train_masks = np.load(path_train_masks, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TlVA_IW0N58W"
      },
      "outputs": [],
      "source": [
        "# remove drive connection as it is no longer needed\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLznZ6dBdhy"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pfM3y1lAfB5"
      },
      "source": [
        "### Machine Learning Regressor\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxlgHGy-CUqv"
      },
      "source": [
        "Suggestions based on [SciKits advice](https://scikit-learn.org/stable/_static/ml_map.png) on choosing the right model:\n",
        "- RidgeRegression\n",
        "- SVR(kernel='linear')\n",
        "- SVR(kernel='rbf')\n",
        "- EnsembleRegressors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Z86jZgBgtW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkMQXJE9AiJV"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGHcRbowAjZv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KovwM0DYBh8a"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mY8ssUvBjLU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}