{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest-height/blob/main/forest_height.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uwfy5ky_YNt7"
      },
      "source": [
        "# Forest Height Estimation\n",
        "Objective of this notebook:\n",
        "- Train model\n",
        "- Evaluate model\n",
        "- Export model for prediction\n",
        "\n",
        "Prerequisite: \n",
        "- Training data is generated in [data_exploration.ipynb](https://github.com/lgiesen/forest-height/blob/main/data_exploration.ipynb)\n",
        "- Prediction of the dataset is realized in a python file (!to be created), but it is tested in a Jupyter Notebook (!to be created)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uwfU-gIiYMDw"
      },
      "source": [
        "### To Dos\n",
        "\n",
        "- Data Augmentation (flip image at least by 90, 180 and 270°)\n",
        "- rename `preds` to `preds_<approach>` -> compare predictions in one overview (also good for poster)\n",
        "- export predictions to file\n",
        "\n",
        "### Tasks from Presentation\n",
        "- Split the dataset into a training and validation set. Train a first regression model on the provided training dataset.\n",
        "- Try out different architectures, hyperparameter.\n",
        "- Now, you are supposed to apply your model to the test set. First, you have to\n",
        "implement the sliding window approach in combination with non-max suppression. Note: Instead of choosing the non-max suppression, you can choose a different\n",
        "approach or come up with your own.\n",
        "- After you have found your best performing setup, apply your model to the unlabeled data set. You can check for plausibility by visually inspecting the output or choose to reuse the predictions for increasing the number of training observations.\n",
        "\n",
        "- implement 1 ML Model (Logistic Regression, Boosted Trees, Random Forest, ...) and one CNN Model -> compare them in a poster (I'd use Figma as a tool)\n",
        "- submission: script that produces a binary output numpy-file (.npy) for every test image, automatically.\n",
        "\n",
        "\n",
        "### Submission\n",
        "- The npy-files should have the same height and width as the original satellite image. Your submission should therefore be a zip file containing multiple npy-files of size [1 × width × weight]\n",
        "- We will use the Mean Absolute Error (MAE) to measure your performance on the hidden test set. For this, we will provide further information at a later time.\n",
        "- Deadline: 4th July, 11:59pm\n",
        "\n",
        "- ZIP-file with all the predictions as described above\n",
        "- ZIP-file with your source code (only the files that are used in your final product)\n",
        "- A poster (A1-size) as pdf-file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57beiuW6Nx-B",
        "outputId": "f01c5ced-93e5-4b3d-fb1b-7373e2bd0fa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive', force_remount=True)\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cDh85SwKRKCe"
      },
      "outputs": [],
      "source": [
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m_Xk6fH-O6_t"
      },
      "outputs": [],
      "source": [
        "def path_exists(path):\n",
        "  import os\n",
        "  return os.path.exists(root_path + path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCgGU0-32oER",
        "outputId": "9c850db0-4dc9-41b9-d02b-417083c3ebed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "satellite: (10, 1024, 1024)\n",
            "mask: (10, 1024, 1024)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# load exemplary data\n",
        "sat_path = 'images/image_004.npy'\n",
        "if path_exists(sat_path):\n",
        "  satellite = np.load(root_path + sat_path)\n",
        "  print('satellite:',satellite.shape)\n",
        "mask_path = 'masks/mask_004.npy'\n",
        "if path_exists(mask_path):\n",
        "  mask = np.load(root_path + mask_path)\n",
        "  mask.shape\n",
        "  print('mask:',satellite.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cMQABaW6Q2pE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_img(img, is_satellite = True):\n",
        "    #shape: satellite == (10, 1024, 1024), mask == (1, 1024, 1024)\n",
        "    if is_satellite:\n",
        "      # Extract Red, Green, and Blue bands\n",
        "      red = img[2, :, :]\n",
        "      green = img[1, :, :]\n",
        "      blue = img[0, :, :]\n",
        "\n",
        "      # Normalize the bands to [0, 1] range\n",
        "      red_norm = (red - red.min()) / (red.max() - red.min())\n",
        "      green_norm = (green - green.min()) / (green.max() - green.min())\n",
        "      blue_norm = (blue - blue.min()) / (blue.max() - blue.min())\n",
        "    \n",
        "      # Stack the bands to create an RGB image\n",
        "      scaled_img = np.stack((red_norm, green_norm, blue_norm), axis=-1)\n",
        "    \n",
        "    elif not is_satellite:\n",
        "      scaled_img = (mask - np.min(mask)) / (np.amax(mask) - np.amin(mask))\n",
        "      # TODO: scale with total max and min of all masks for comparability\n",
        "      scaled_img = np.squeeze(scaled_img) # remove redundant dimension\n",
        "      \n",
        "      \n",
        "    # Plot the image\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(scaled_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s1lFuJ_jWi5m"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vtSyrAb3-pku"
      },
      "source": [
        "The npy files are combined into a dataset. After the first loading they do not have to be generated anymore.\n",
        "One npy file might need to be split into smaller images of 256x256 or 512x512 pixels, which then are put back together in the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aZ_VFutfOgFP"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "path_train_sat = f\"{root_path}train_satellite.npy\"\n",
        "path_train_masks = f\"{root_path}train_masks.npy\"\n",
        "\n",
        "X = np.load(path_train_sat, allow_pickle=True)\n",
        "y = np.load(path_train_masks, allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TlVA_IW0N58W"
      },
      "outputs": [],
      "source": [
        "# remove drive connection as it is no longer needed\n",
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Z86jZgBgtW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split dataset for training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLznZ6dBdhy"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0pfM3y1lAfB5"
      },
      "source": [
        "### Machine Learning Regressor\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zxlgHGy-CUqv"
      },
      "source": [
        "Suggestions based on [SciKits advice](https://scikit-learn.org/stable/_static/ml_map.png) on choosing the right model:\n",
        "- RidgeRegression\n",
        "- SVR(kernel='linear')\n",
        "- SVR(kernel='rbf')\n",
        "- EnsembleRegressors"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Random Forest Tipps from Lecture:\n",
        "\n",
        "1. Trees & Arrays: Each tree is stored in an array (low-level, C-like)\n",
        "2. Sorting: Optimized sorting algorithm (standard qsort → factor 2-3 slower)\n",
        "3. Locally constant features: Keep track of “constant” features during construction.\n",
        "4. Unique instances: If bootstrap=True, only use unique indices/patterns (only 63%) and assign weights to them → Speed-up of about 1.5!\n",
        "5. Consecutive memory access: For evaluating O(S), prefetch data for dimension i\n",
        "6. Sparse data: Make use of optimized computations for sparse data.\n",
        "7. Random numbers: Use manual random number generator . . .\n",
        "8. Parallelization: For random forests, efficient parallelization over trees."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# instantiate the model\n",
        "# TODO: Try out different values for min_samples_split\n",
        "# corresponds to \"m\" in the slides\n",
        "min_samples_split = 2\n",
        "\n",
        "# instantiate model and fit it!\n",
        "model = DecisionTreeRegressor(max_depth=2, min_samples_split=min_samples_split)\n",
        "model_2 = DecisionTreeRegressor(max_depth=5, min_samples_split=min_samples_split)\n",
        "\n",
        "# train the model\n",
        "model.fit(X, y)\n",
        "model_2.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict\n",
        "# X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
        "preds = model.predict(X_test)\n",
        "preds_model_2 = model_2.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "plt.figure()\n",
        "plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
        "plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n",
        "plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n",
        "plt.xlabel(\"data\")\n",
        "plt.ylabel(\"target\")\n",
        "plt.title(\"Decision Tree Regression\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# from the Lecture:\n",
        "X_plot = np.arange(0.0, 5.0, 0.01).reshape((-1,1))\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(X, y, c=\"darkorange\", label=\"Data\", s=25)\n",
        "plt.plot(X_plot, preds, color=\"cornflowerblue\", label=\"Model\", linewidth=3)\n",
        "plt.xlabel(\"X\", fontsize=18)\n",
        "plt.ylabel(\"Y\", fontsize=18)\n",
        "plt.title(f\"Regression Tree (m={min_samples_split})\", fontsize=16)\n",
        "plt.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest Regression\n",
        "https://towardsdatascience.com/machine-learning-basics-random-forest-regression-be3e1e3bb91a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# initialize model\n",
        "model = RandomForestRegressor(n_estimators=100)\n",
        "# train model\n",
        "model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predict values\n",
        "preds = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# compute RMSE\n",
        "print(\"RMSE: {}\".format(np.sqrt(mean_squared_error(y_test, preds))))\n",
        "\n",
        "# visualize predictions vs. true labels\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.scatter(preds, y_test, color=\"blue\", alpha=0.5)\n",
        "plt.xticks(rotation=45)\n",
        "plt.gca().xaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
        "plt.plot([-100000,600000], [-100000, 600000], 'k--')\n",
        "plt.xlabel(\"Predictions\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.xlim([-100000,600000])\n",
        "plt.ylim([-100000,600000])\n",
        "plt.title(\"Evaluation of Random Forest Regression Model\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Support Vector Regression\n",
        "\n",
        "https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BkMQXJE9AiJV"
      },
      "source": [
        "### Convolutional Neural Network (CNN)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KovwM0DYBh8a"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mY8ssUvBjLU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
