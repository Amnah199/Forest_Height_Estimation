{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest_height/blob/main/notebooks/0_generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "u73c0Q_dtc2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "drive.mount ('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaXIeWlctulu",
        "outputId": "2c6bb7e8-2eda-4fca-84d9-3bc713d55123"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the zipped data is uploaded in the root_path folder\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'\n",
        "user = \"lgiesen\"\n",
        "repo = \"forest_height\"\n",
        "!git clone https://github.com/{user}/{repo}.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHOyEgCG4vju",
        "outputId": "1d70b137-109f-464a-da84-1a6efe85b955"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'forest_height' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/forest_height/src/generate_data.py"
      ],
      "metadata": {
        "id": "nNDKskC-hrBu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_files = [filename for filename in get_files(root_path) if '.zip' in filename]\n",
        "zip_files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfEqJ-bvhmSa",
        "outputId": "46f130b5-4027-4099-976f-2fa61721d6c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['masks_train.zip', 'images_train.zip', 'masks_02.zip', 'images_02.zip']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# unzip data\n",
        "%cd \"drive/MyDrive/Colab Notebooks/data/\"\n",
        "# use -B flag to rename files if there is a file with its name\n",
        "!for f in *.zip; do unzip -B \"$f\"; done\n",
        "%cd ../../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j6QFmDk78E9",
        "outputId": "b3971dd4-28c1-499a-ec7f-7da5f0f15323"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data\n",
            "Archive:  images_02.zip\n",
            "  inflating: images/image_000.npy    \n",
            "  inflating: images/image_001.npy    \n",
            "  inflating: images/image_002.npy    \n",
            "  inflating: images/image_003.npy    \n",
            "  inflating: images/image_004.npy    \n",
            "  inflating: images/image_005.npy    \n",
            "  inflating: images/image_006.npy    \n",
            "  inflating: images/image_007.npy    \n",
            "  inflating: images/image_008.npy    \n",
            "  inflating: images/image_009.npy    \n",
            "  inflating: images/image_010.npy    \n",
            "  inflating: images/image_011.npy    \n",
            "  inflating: images/image_012.npy    \n",
            "  inflating: images/image_013.npy    \n",
            "  inflating: images/image_014.npy    \n",
            "  inflating: images/image_015.npy    \n",
            "  inflating: images/image_016.npy    \n",
            "  inflating: images/image_017.npy    \n",
            "  inflating: images/image_018.npy    \n",
            "  inflating: images/image_019.npy    \n",
            "Archive:  images_train.zip\n",
            "  inflating: images/image_000.npy    \n",
            "  inflating: images/image_001.npy    \n",
            "  inflating: images/image_002.npy    \n",
            "  inflating: images/image_003.npy    \n",
            "  inflating: images/image_004.npy    \n",
            "  inflating: images/image_005.npy    \n",
            "  inflating: images/image_006.npy    \n",
            "  inflating: images/image_007.npy    \n",
            "  inflating: images/image_008.npy    \n",
            "  inflating: images/image_009.npy    \n",
            "  inflating: images/image_010.npy    \n",
            "  inflating: images/image_011.npy    \n",
            "  inflating: images/image_012.npy    \n",
            "  inflating: images/image_013.npy    \n",
            "  inflating: images/image_014.npy    \n",
            "  inflating: images/image_015.npy    \n",
            "  inflating: images/image_016.npy    \n",
            "  inflating: images/image_017.npy    \n",
            "  inflating: images/image_018.npy    \n",
            "  inflating: images/image_019.npy    \n",
            "Archive:  masks_02.zip\n",
            "  inflating: masks/mask_000.npy      \n",
            "  inflating: masks/mask_001.npy      \n",
            "  inflating: masks/mask_002.npy      \n",
            "  inflating: masks/mask_003.npy      \n",
            "  inflating: masks/mask_004.npy      \n",
            "  inflating: masks/mask_005.npy      \n",
            "  inflating: masks/mask_006.npy      \n",
            "  inflating: masks/mask_007.npy      \n",
            "  inflating: masks/mask_008.npy      \n",
            "  inflating: masks/mask_009.npy      \n",
            "  inflating: masks/mask_010.npy      \n",
            "  inflating: masks/mask_011.npy      \n",
            "  inflating: masks/mask_012.npy      \n",
            "  inflating: masks/mask_013.npy      \n",
            "  inflating: masks/mask_014.npy      \n",
            "  inflating: masks/mask_015.npy      \n",
            "  inflating: masks/mask_016.npy      \n",
            "  inflating: masks/mask_017.npy      \n",
            "  inflating: masks/mask_018.npy      \n",
            "  inflating: masks/mask_019.npy      \n",
            "Archive:  masks_train.zip\n",
            "  inflating: masks/mask_000.npy      \n",
            "  inflating: masks/mask_001.npy      \n",
            "  inflating: masks/mask_002.npy      \n",
            "  inflating: masks/mask_003.npy      \n",
            "  inflating: masks/mask_004.npy      \n",
            "  inflating: masks/mask_005.npy      \n",
            "  inflating: masks/mask_006.npy      \n",
            "  inflating: masks/mask_007.npy      \n",
            "  inflating: masks/mask_008.npy      \n",
            "  inflating: masks/mask_009.npy      \n",
            "  inflating: masks/mask_010.npy      \n",
            "  inflating: masks/mask_011.npy      \n",
            "  inflating: masks/mask_012.npy      \n",
            "  inflating: masks/mask_013.npy      \n",
            "  inflating: masks/mask_014.npy      \n",
            "  inflating: masks/mask_015.npy      \n",
            "  inflating: masks/mask_016.npy      \n",
            "  inflating: masks/mask_017.npy      \n",
            "  inflating: masks/mask_018.npy      \n",
            "  inflating: masks/mask_019.npy      \n",
            "/content\n",
            "CPU times: user 361 ms, sys: 48.6 ms, total: 409 ms\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from zipfile import ZipFile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def get_files(dir):\n",
        "    \"\"\"\n",
        "    Get all files from a directory\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dir: Array of strings\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Array of strings\n",
        "    \"\"\"\n",
        "    return [f for f in listdir(dir) if isfile(join(dir, f))]\n",
        "\n",
        "def extract_data(data_filenames):\n",
        "    \"\"\"\n",
        "    Extract data from zipped files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_filenames: Array of strings\n",
        "    Path to the train data (default: None)\n",
        "    root_path + filename = complete filepath\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset: Tuple of np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    # load satellite images by loading the first one and then concatenating the rest\n",
        "    X = np.load(f'{path_images}{get_files(path_images)[0]}')\n",
        "    for filename in get_files(path_images)[1:]:\n",
        "        temp = np.load(f'{path_images}{filename}', allow_pickle=True)\n",
        "        X = np.concatenate((X, temp))\n",
        "    # reshape X to distinguish between image and color channel\n",
        "    num_imgs = len(get_files(path_images))\n",
        "    X = X.reshape((num_imgs, int(X.shape[0]/num_imgs), X.shape[1], X.shape[2]))\n",
        "    # ceil the values at 2000 because clouds have a different reflection value\n",
        "    ceiling = 2000\n",
        "    X[X > ceiling] = ceiling\n",
        "    #scale values between 0 and 1\n",
        "    X = X / ceiling\n",
        "\n",
        "    # load labels by loading the first one and then concatenating the rest\n",
        "    y = np.load(f'{path_masks}{get_files(path_masks)[0]}')\n",
        "    for filename in get_files(path_masks)[1:]:\n",
        "        temp = np.load(f'{path_masks}{filename}', allow_pickle=True)\n",
        "        y = np.concatenate((y, temp))\n",
        "\n",
        "    del temp, ceiling, num_imgs\n",
        "\n",
        "    return (X, y)\n",
        "\n",
        "def extract_labels(X, y):\n",
        "    \"\"\"\n",
        "    Labels are sparse, so they are Get all labels (non-zero elements) from a set of images\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: numpy.ndarray\n",
        "    y: numpy.ndarray\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df: pandas.DataFrame\n",
        "    \"\"\"\n",
        "    # extract non-zero value indices from y (= label position) to extract the corresponding X-value\n",
        "    # prepare data to merge it into one data frame,\n",
        "    # which makes it easier to extract the values of the same pixel\n",
        "    X = X.reshape(10, -1)\n",
        "    y = y.reshape(1, -1)\n",
        "    Xy = np.concatenate((X, y), axis=0)\n",
        "    Xy = Xy.transpose()\n",
        "    data = np.empty((0,11))\n",
        "    data = np.concatenate((data, Xy), axis=0)\n",
        "\n",
        "    indices = np.nonzero(data[:,-1])\n",
        "    labeled_data = data[indices]\n",
        "\n",
        "    # create dataframe with features and labels\n",
        "    df = pd.DataFrame(labeled_data)\n",
        "    column_names = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'Label']\n",
        "    df.columns = column_names\n",
        "\n",
        "    return df\n",
        "\n",
        "def upsample_data(df):\n",
        "    \"\"\"\n",
        "    Upsample underrepresented data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pandas.DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features: pandas.DataFrame\n",
        "    labels: pandas.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # sort data according to tree height asc\n",
        "    dfs = df.sort_values('Label').reset_index(drop=True)\n",
        "    # create empty data frame to fill\n",
        "    dff = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    index_start = 0\n",
        "    for i in range(3, 37, 3):\n",
        "      #count the number of intances that are in one interval for example 0 - 3 or 15 - 18\n",
        "      index_end = index_start + dfs[\"Label\"][(dfs[\"Label\"] > i - 3) & (dfs[\"Label\"] < i)].count()\n",
        "      # take random smaple of the interval\n",
        "      samp = dfs[index_start:index_end].sample(800)\n",
        "      dff = pd.concat((dff, samp))\n",
        "      index_start = index_end\n",
        "\n",
        "    # add the highest values beacuase there are only a few\n",
        "    dff = pd.concat((dff, dfs[index_start:]))\n",
        "    dftr = dff.sample(frac=1).reset_index(drop=True) #shuffel the dataset randomly\n",
        "\n",
        "    # extract features and labels\n",
        "    features = dftr.iloc[:, 0:10]\n",
        "    labels = dftr.iloc[:,10]\n",
        "\n",
        "    # the length of X and y has to be the same\n",
        "    # assert features.shape[0] == labels.shape[0]\n",
        "    return (features, labels)\n",
        "\n",
        "def generate_dataset(zip_files):\n",
        "    \"\"\"\n",
        "    Generate a dataset (X_train, X_test, y_train, y_test) based on the location of zip files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    zip_files: Array of strings\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Numpy.ndarray\n",
        "    \"\"\"\n",
        "    X, y = extract_data(zip_files)\n",
        "    del zip_files\n",
        "    df = extract_labels(X, y)\n",
        "    del X, y\n",
        "    features, labels = upsample_data(df)\n",
        "    del df\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0, shuffle=True)\n",
        "    del features, labels\n",
        "    return (X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "l5S7cprnPyfU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_train, X_test, y_train, y_test = generate_dataset(zip_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8N-yB3Ll_4m",
        "outputId": "ae032c0c-2512-4c40-a034-7ba63ef46705"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17.5 s, sys: 28.4 s, total: 45.9 s\n",
            "Wall time: 52.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove drive connection as it is no longer needed\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "wJ1a2sqeJhg1"
      },
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}