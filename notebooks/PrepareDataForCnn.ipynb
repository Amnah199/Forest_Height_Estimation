{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONxWGE/OtiZwoRPNfT/oKF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest_height/blob/main/notebooks/PrepareDataForCnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuoUbjINM7AY",
        "outputId": "52c820a7-74b4-4b25-855b-64e2d904aa80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, os, cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, InputLayer, Flatten, Conv2D, MaxPool2D\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint"
      ],
      "metadata": {
        "id": "u2B4lNUxNFCp"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa2/images_train.zip #first part of the data"
      ],
      "metadata": {
        "id": "K6-UZHTMNFv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa2/masks_train.zip"
      ],
      "metadata": {
        "id": "fX3VLTDyNI0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load all the data and extract all features and labes"
      ],
      "metadata": {
        "id": "Orhaa12VJJzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 5 #define window size should be odd so that the label is in the middle\n",
        "shape = (10, size, size) #define shape of features\n",
        "labels1 = np.ones(1) #array for labels\n",
        "data1 = np.ones(shape) #array for features\n",
        "data1 = np.expand_dims(data1, axis=0) #expand dimension to concatenate\n",
        "for j in range(20): #iterate over images in directory\n",
        "  if j < 10:\n",
        "    X = np.load('/content/images/image_00'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_00'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0) #select all values with label\n",
        "    indices_2d = indices[:, 1:] #extract indices\n",
        "    ind_y = np.ones(2).reshape(-1,2) #array to collect indices\n",
        "    for i in indices_2d: #iterate over indices\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape: #select only features with the same shape because of labels at the image border\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1] #save them temporary\n",
        "        temp2 = np.expand_dims(temp, axis=0) #expand dimension to concatenate\n",
        "        data1 = np.concatenate((data1, temp2), axis=0) #concatenation\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0) #concatenation of index so that they have the same order and length as the features\n",
        "\n",
        "    ind_y = ind_y[1:] #remove first dummy values\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2] #extract labels\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0) #concatenate labels\n",
        "\n",
        "  if j >= 10:\n",
        "    X = np.load('/content/images/image_0'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_0'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0)\n",
        "    indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    for i in indices_2d:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    ind_y = ind_y[1:]\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2]\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "data1 = data1[1:] #remove first dummy values\n",
        "labels1 = labels1[1:] #remove first dummy values"
      ],
      "metadata": {
        "id": "5Ta4YC51NLff"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa22/images_02.zip #second part of the data"
      ],
      "metadata": {
        "id": "WT-YqP_XNXOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa22/masks_02.zip"
      ],
      "metadata": {
        "id": "dW0Ms1npNczW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 5\n",
        "shape = (10, size, size)\n",
        "labels2 = np.ones(1)\n",
        "data2 = np.ones(shape)\n",
        "data2 = np.expand_dims(data2, axis=0)\n",
        "for j in range(20):\n",
        "  if j < 10:\n",
        "    X = np.load('/content/images/image_00'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_00'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0)\n",
        "    indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    for i in indices_2d:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data2 = np.concatenate((data2, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    ind_y = ind_y[1:]\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2]\n",
        "    labels2 = np.concatenate((labels2, data_y), axis = 0)\n",
        "\n",
        "  if j >= 10:\n",
        "    X = np.load('/content/images/image_0'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_0'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0)\n",
        "    indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    for i in indices_2d:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data2 = np.concatenate((data2, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    ind_y = ind_y[1:]\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2]\n",
        "    labels2 = np.concatenate((labels2, data_y), axis = 0)\n",
        "\n",
        "data2 = data2[1:]\n",
        "labels2 = labels2[1:]"
      ],
      "metadata": {
        "id": "rgCVA7wMNfye"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.concatenate((data1, data2), axis = 0) #concatenate both parts\n",
        "labels = np.concatenate((labels1, labels2), axis = 0)"
      ],
      "metadata": {
        "id": "xYi1H5aUOQg-"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijDuB0GIQxgG",
        "outputId": "889ddbf2-183a-4549-90d6-94a5561d1965"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38531, 10, 5, 5)\n",
            "(38531,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scale data"
      ],
      "metadata": {
        "id": "tYNTQQP5YeR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ceiling = 2000\n",
        "features2 = features\n",
        "features2[features2 > ceiling] = ceiling\n",
        "features2 = features2/ceiling\n",
        "labels2 = labels/ceiling"
      ],
      "metadata": {
        "id": "S2sXBmp3Qs8o"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ceiling = 2000\n",
        "features3 = features\n",
        "features3[features3 > ceiling] = ceiling\n",
        "features3 = (features3 - np.mean(features3)) / np.std(features3)** (1/2) #normalize features\n",
        "labels3 = (labels - np.mean(labels)) / np.std(labels)** (1/2)"
      ],
      "metadata": {
        "id": "0zfdZ3p7RXyj"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create equally distributed training *data*"
      ],
      "metadata": {
        "id": "pzWewwJdJVZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 800\n",
        "\n",
        "num = (list(range(3, 37, 3))) #create list from 3 to 36 step 3\n",
        "shape = (10, 5, 5)\n",
        "data_bal = np.ones(shape) #create array to fill with features\n",
        "data_bal = np.expand_dims(data_bal, axis=0) #expand one dimension to concatenate\n",
        "data_lab = np.ones(1) #create array to fill labels\n",
        "for i in num:\n",
        "  indices = np.where((labels > i-3) & (labels <= i)) #select indcies from every 3 meter interval until 36\n",
        "  sampled_indices = np.random.choice(indices[0].flatten(), size=sample_size, replace=False) #random sample of each interval\n",
        "  tempx = features[sampled_indices]\n",
        "  tempy = labels[sampled_indices]\n",
        "  data_bal = np.concatenate((data_bal, tempx), axis=0)\n",
        "  data_lab = np.concatenate((data_lab, tempy), axis=0)\n",
        "\n",
        "indices = np.where((labels > 36)) #add the values > 36 m, they are so few no sample needed\n",
        "sampled_indices = indices[0].flatten()\n",
        "tempx = features[sampled_indices]\n",
        "tempy = labels[sampled_indices]\n",
        "data_bal = np.concatenate((data_bal[1:], tempx), axis=0)\n",
        "data_lab = np.concatenate((data_lab[1:], tempy), axis=0)\n",
        "data_bal2 = data_bal\n",
        "data_bal2[data_bal2 > ceiling] = ceiling\n",
        "data_bal2 = (data_bal2 - np.mean(data_bal2)) / np.std(data_bal2)** (1/2)\n"
      ],
      "metadata": {
        "id": "VcR2jefRJuie"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_bal2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ4InylhKgAI",
        "outputId": "131c1191-fb37-4f92-cc19-eafa648862f7"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10488, 10, 5, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split"
      ],
      "metadata": {
        "id": "LBoNTcM5Jg_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(data_bal, data_lab , test_size = 0.3, random_state=3) #create train, test set"
      ],
      "metadata": {
        "id": "6IeXAw6zRBki"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and evaluate Cnn"
      ],
      "metadata": {
        "id": "n2ePhptk9zzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "lyPnIFFtnD1d"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCnn = Sequential() #bulid cnn\n",
        "modelCnn.add(InputLayer(input_shape = (10, 5, 5)))\n",
        "modelCnn.add(Conv2D(filters=50, kernel_size= (3,3), strides=  1 , padding = \"valid\", activation='relu' )) #padding valid\n",
        "#modelCnn.add(Conv2D(filters=100, kernel_size= (3,3), strides=  1 , padding = \"valid\", activation='relu' ))\n",
        "modelCnn.add(MaxPool2D(pool_size = (2,2)))\n",
        "modelCnn.add(Flatten())\n",
        "modelCnn.add(Dense(400, activation='relu'))\n",
        "modelCnn.add(Dense(200, activation='relu'))\n",
        "#modelCnn.add(Dropout(0.25))\n",
        "modelCnn.add(Dense(100, activation='relu'))\n",
        "modelCnn.add(Dropout(0.4))\n",
        "modelCnn.add(Dense(1, activation='linear'))\n",
        "\n",
        "modelCnn.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rInXy7f9lLAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCnn.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_percentage_error']) #compile cnn\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "model_save = ModelCheckpoint(\"/content/gdrive/MyDrive/NNmodels/best_modelCnnSmallStdBal.hdf5\", save_best_only = True) #save best model\n",
        "\n",
        "modelCnn.fit(Xtrain, ytrain, epochs = 100, validation_data=(Xtest, ytest), callbacks=[tensorboard_callback, model_save]) #train cnn  batch_size = 32"
      ],
      "metadata": {
        "id": "Bc9Eh2yTRoRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmodel = load_model('/content/gdrive/MyDrive/NNmodels/best_modelCnnSmallStdBal.hdf5')\n",
        "ypred_nn = bmodel.predict(Xtest) #predict best model\n",
        "\n",
        "\n",
        "mse_nn = mse(ytest, ypred_nn) #calculate metrics\n",
        "rmse_nn = mse_nn ** (1/2)\n",
        "mae_nn = mae(ytest, ypred_nn)\n",
        "mape_nn = mape(ytest, ypred_nn)\n",
        "\n",
        "print(mape_nn)\n",
        "print(mae_nn)\n",
        "print(rmse_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BNnwUZMRsfp",
        "outputId": "358525c0-d520-466b-98e8-76b8059ba2bb"
      },
      "execution_count": 227,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99/99 [==============================] - 1s 7ms/step\n",
            "0.4586004335747886\n",
            "6.028142741569686\n",
            "8.379354940878384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "aM160duInNR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_nn = np.mean(ypred_nn[:]) #calculate mean\n",
        "quantiles_nn = np.percentile(ypred_nn[:], [1, 25, 50, 75, 99]) #calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "mean_labels = np.mean(data_lab[:])\n",
        "quantiles_labels = np.percentile(data_lab[:], [1, 25, 50, 75, 99])\n",
        "\n",
        "print(mean_nn)\n",
        "print(quantiles_nn)\n",
        "print(np.sort(ypred_nn.flatten())[:10]) #print the 10 lowest predictions\n",
        "print(np.sort(ypred_nn.flatten())[-10:][::-1]) #print the 10 highest predictions\n",
        "\n",
        "print(mean_labels)\n",
        "print(quantiles_labels)\n",
        "print(np.sort(data_lab.flatten())[:10])\n",
        "print(np.sort(data_lab.flatten())[-10:][::-1])"
      ],
      "metadata": {
        "id": "a1cgkn6sVuHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict image with sliding window"
      ],
      "metadata": {
        "id": "UHh4Hms3-BU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmodel = load_model('/content/gdrive/MyDrive/NNmodels/best_modelCnnSSmall.hdf5')"
      ],
      "metadata": {
        "id": "j2DyNQsg-pqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xs = np.load('/content/images/image_000.npy') #takes a long time depending on the size of the image\n",
        "#Xsc = Xs[:, :64, 1024-64 :] #crop smaller subimage\n",
        "Xsc = (Xsc - np.mean(Xsc)) / np.std(Xsc)** (1/2) #normalize features\n",
        "height, width = Xsc.shape[1], Xsc.shape[2] #get size of image\n",
        "window_size = 5\n",
        "predicted_image = np.ones((1, height, width)) #create image to fill\n",
        "\n",
        "for h in range(height - window_size + 1):\n",
        "    for w in range(width - window_size + 1):\n",
        "        window = Xsc[:, h:h+window_size, w:w+window_size]\n",
        "        window_reshaped = np.expand_dims(window, axis=0)  # Reshape the window to match the expected input shape\n",
        "        ypred = tmodel.predict(window_reshaped)\n",
        "\n",
        "        # Assign the predicted value to the corresponding position in the predicted_image array\n",
        "        predicted_image[:, h, w] = ypred\n"
      ],
      "metadata": {
        "id": "dLC3mRB39_q7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}