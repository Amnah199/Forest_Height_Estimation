{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGyvGSeeH5XOFa3Lo6MO33",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest_height/blob/main/notebooks/PrepareDataForCnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuoUbjINM7AY",
        "outputId": "52c820a7-74b4-4b25-855b-64e2d904aa80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, os, cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.ticker import StrMethodFormatter\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, mean_absolute_percentage_error as mape\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, BatchNormalization, Dropout, InputLayer, Flatten, Conv2D, MaxPool2D\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint"
      ],
      "metadata": {
        "id": "u2B4lNUxNFCp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa2/images_train.zip #first part of the data"
      ],
      "metadata": {
        "id": "K6-UZHTMNFv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa2/masks_train.zip"
      ],
      "metadata": {
        "id": "fX3VLTDyNI0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 5 #define window size should be odd so that the label is in the middle\n",
        "shape = (10, size, size) #define shape of features\n",
        "labels1 = np.ones(1) #array for labels\n",
        "data1 = np.ones(shape) #array for features\n",
        "data1 = np.expand_dims(data1, axis=0) #expand dimension to concatenate\n",
        "for j in range(20): #iterate over images in directory\n",
        "  if j < 10:\n",
        "    X = np.load('/content/images/image_00'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_00'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0) #select all values with label\n",
        "    indices_2d = indices[:, 1:] #extract indices\n",
        "    ind_y = np.ones(2).reshape(-1,2) #array to collect indices\n",
        "    for i in indices_2d: #iterate over indices\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape: #select only features with the same shape because of labels at the image border\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1] #save them temporary\n",
        "        temp2 = np.expand_dims(temp, axis=0) #expand dimension to concatenate\n",
        "        data1 = np.concatenate((data1, temp2), axis=0) #concatenation\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0) #concatenation of index so that they have the same order and length as the features\n",
        "\n",
        "    ind_y = ind_y[1:] #remove first dummy values\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2] #extract labels\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0) #concatenate labels\n",
        "\n",
        "  if j >= 10:\n",
        "    X = np.load('/content/images/image_0'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_0'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0)\n",
        "    indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    for i in indices_2d:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data1 = np.concatenate((data1, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    ind_y = ind_y[1:]\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2]\n",
        "    labels1 = np.concatenate((labels1, data_y), axis = 0)\n",
        "\n",
        "data1 = data1[1:] #remove first dummy values\n",
        "labels1 = labels1[1:] #remove first dummy values"
      ],
      "metadata": {
        "id": "5Ta4YC51NLff"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa22/images_02.zip #second part of the data"
      ],
      "metadata": {
        "id": "WT-YqP_XNXOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip gdrive/My\\ Drive/DataDa22/masks_02.zip"
      ],
      "metadata": {
        "id": "dW0Ms1npNczW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size = 5\n",
        "shape = (10, size, size)\n",
        "labels2 = np.ones(1)\n",
        "data2 = np.ones(shape)\n",
        "data2 = np.expand_dims(data2, axis=0)\n",
        "for j in range(20):\n",
        "  if j < 10:\n",
        "    X = np.load('/content/images/image_00'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_00'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0)\n",
        "    indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    for i in indices_2d:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data2 = np.concatenate((data2, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    ind_y = ind_y[1:]\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2]\n",
        "    labels2 = np.concatenate((labels2, data_y), axis = 0)\n",
        "\n",
        "  if j >= 10:\n",
        "    X = np.load('/content/images/image_0'+ str(j) + '.npy')\n",
        "    y = np.load('/content/masks/mask_0'+ str(j) + '.npy')\n",
        "    indices = np.argwhere(y > 0)\n",
        "    indices_2d = indices[:, 1:]\n",
        "    ind_y = np.ones(2).reshape(-1,2)\n",
        "    for i in indices_2d:\n",
        "      if shape == X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1].shape:\n",
        "        temp = X[:, i[0] - (size//2):i[0] + (size//2) + 1, i[1] - (size//2):i[1] + (size//2) + 1]\n",
        "        temp2 = np.expand_dims(temp, axis=0)\n",
        "        data2 = np.concatenate((data2, temp2), axis=0)\n",
        "\n",
        "        ind_y = np.concatenate((ind_y, i.reshape(-1,2)), axis=0)\n",
        "\n",
        "    ind_y = ind_y[1:]\n",
        "    indices_1 = ind_y[:, 0].astype(int)\n",
        "    indices_2 = ind_y[:, 1].astype(int)\n",
        "    data_y = y[0, indices_1, indices_2]\n",
        "    labels2 = np.concatenate((labels2, data_y), axis = 0)\n",
        "\n",
        "data2 = data2[1:]\n",
        "labels2 = labels2[1:]"
      ],
      "metadata": {
        "id": "rgCVA7wMNfye"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = np.concatenate((data1, data2), axis = 0) #concatenate both parts\n",
        "labels = np.concatenate((labels1, labels2), axis = 0)"
      ],
      "metadata": {
        "id": "xYi1H5aUOQg-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(features.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijDuB0GIQxgG",
        "outputId": "889ddbf2-183a-4549-90d6-94a5561d1965"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(38531, 10, 5, 5)\n",
            "(38531,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = train_test_split(features, labels , test_size = 0.3, random_state=3) #create train, test set"
      ],
      "metadata": {
        "id": "6IeXAw6zRBki"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "lyPnIFFtnD1d"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCnn = Sequential() #bulid cnn\n",
        "modelCnn.add(InputLayer(input_shape = (10, 5, 5)))\n",
        "modelCnn.add(Conv2D(filters=50, kernel_size= (3,3), strides=  1 , padding = \"valid\", activation='relu' )) #padding valid\n",
        "modelCnn.add(Conv2D(filters=100, kernel_size= (3,3), strides=  1 , padding = \"valid\", activation='relu' ))\n",
        "modelCnn.add(Flatten())\n",
        "modelCnn.add(Dense(100, activation='relu'))\n",
        "modelCnn.add(Dense(50, activation='relu'))\n",
        "modelCnn.add(Dense(1, activation='linear'))\n",
        "\n",
        "modelCnn.summary()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rInXy7f9lLAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelCnn.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_percentage_error']) #compile cnn\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "model_save = ModelCheckpoint(\"/content/gdrive/MyDrive/NNmodels/best_model.hdf5\", save_best_only = True) #save best model\n",
        "\n",
        "modelCnn.fit(Xtrain, ytrain, epochs = 50, validation_data=(Xtest, ytest), callbacks=[tensorboard_callback, model_save], batch_size = 128) #train cnn"
      ],
      "metadata": {
        "id": "Bc9Eh2yTRoRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bmodel = load_model('/content/gdrive/MyDrive/NNmodels/best_model.hdf5')\n",
        "ypred_nn = bmodel.predict(Xtest) #predict best model\n",
        "\n",
        "\n",
        "mse_nn = mse(ytest, ypred_nn) #calculate metrics\n",
        "rmse_nn = mse_nn ** (1/2)\n",
        "mae_nn = mae(ytest, ypred_nn)\n",
        "mape_nn = mape(ytest, ypred_nn)\n",
        "\n",
        "print(mape_nn)\n",
        "print(mae_nn)\n",
        "print(rmse_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BNnwUZMRsfp",
        "outputId": "31605946-f80d-40ad-cb1d-a77a06fcf154"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "362/362 [==============================] - 2s 4ms/step\n",
            "0.47492916277202396\n",
            "4.265573303880989\n",
            "7.078569066210429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "aM160duInNR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_nn = np.mean(ypred_nn[:]) #calculate mean\n",
        "quantiles_nn = np.percentile(ypred_nn[:], [1, 25, 50, 75, 99]) #calculate quantiles 0.01, 0.25, 0.5, 0.75, 0.99\n",
        "mean_labels = np.mean(labels[:])\n",
        "quantiles_labels = np.percentile(labels[:], [1, 25, 50, 75, 99])\n",
        "\n",
        "print(mean_nn)\n",
        "print(quantiles_nn)\n",
        "print(np.sort(ypred_nn.flatten())[:10]) #print the 10 lowest predictions\n",
        "print(np.sort(ypred_nn.flatten())[-10:][::-1]) #print the 10 highest predictions\n",
        "\n",
        "print(mean_labels)\n",
        "print(quantiles_labels)\n",
        "print(np.sort(labels.flatten())[:10])\n",
        "print(np.sort(labels.flatten())[-10:][::-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1cgkn6sVuHT",
        "outputId": "1980a0b3-1bb1-488d-cbeb-6713791abecb"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.973388\n",
            "[ 2.72695328  3.00690836  4.7546308  20.26180363 28.90485104]\n",
            "[1.3616598 1.687053  1.8989965 2.0729659 2.129307  2.2452602 2.3526623\n",
            " 2.3584313 2.3638072 2.417257 ]\n",
            "[50.100033 39.6519   38.58132  34.773563 33.398247 32.901283 32.515965\n",
            " 32.32575  31.560488 31.305954]\n",
            "11.88373569284665\n",
            "[ 2.3900001   2.8599999   5.6500001  20.79999924 39.44700012]\n",
            "[1.34000003 1.57000005 1.75       1.83000004 1.89999998 1.89999998\n",
            " 1.94000006 1.98000002 2.16000009 2.17000008]\n",
            "[81.69000244 69.97000122 62.13000107 58.63000107 57.93000031 57.56000137\n",
            " 55.20000076 54.56000137 54.11999893 54.04999924]\n"
          ]
        }
      ]
    }
  ]
}