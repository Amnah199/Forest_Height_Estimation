{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lgiesen/forest_height/blob/main/notebooks/generate_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "u73c0Q_dtc2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaXIeWlctulu",
        "outputId": "d5c70494-e8c5-4635-d7b0-09b95fabf640"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the zipped data is uploaded in the root_path folder\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'\n",
        "user = \"lgiesen\"\n",
        "repo = \"forest_height\"\n",
        "!git clone https://github.com/{user}/{repo}.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHOyEgCG4vju",
        "outputId": "c0939123-e406-4d95-d575-27b5af5d9a01"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'forest_height' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/forest_height/src/generate_data.py"
      ],
      "metadata": {
        "id": "nNDKskC-hrBu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "root_path = 'drive/MyDrive/Colab Notebooks/data/'\n",
        "path_images = f'{root_path}images/'\n",
        "path_masks = f'{root_path}masks/'\n",
        "\n",
        "\n",
        "def get_files(dir):\n",
        "    \"\"\"\n",
        "    Get all files from a directory\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dir: Array of strings\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Array of strings\n",
        "    \"\"\"\n",
        "    return [f for f in listdir(dir) if isfile(join(dir, f))]\n",
        "\n",
        "def extract_data(path_images, path_masks):\n",
        "    \"\"\"\n",
        "    Extract data from zipped files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_images: String\n",
        "    path_masks: String\n",
        "    Path to the train data (default: None)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dataset: Tuple of np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    # load satellite images by loading the first one and then concatenating the rest\n",
        "    X = np.load(f'{path_images}{get_files(path_images)[0]}')\n",
        "    for filename in get_files(path_images)[1:]:\n",
        "        temp = np.load(f'{path_images}{filename}', allow_pickle=True)\n",
        "        X = np.concatenate((X, temp))\n",
        "    # reshape X to distinguish between image and color channel\n",
        "    num_imgs = len(get_files(path_images))\n",
        "    X = X.reshape((num_imgs, int(X.shape[0]/num_imgs), X.shape[1], X.shape[2]))\n",
        "    # ceil the values at 2000 because clouds have a different reflection value\n",
        "    ceiling = 2000\n",
        "    X[X > ceiling] = ceiling\n",
        "    #scale values between 0 and 1\n",
        "    X = X / ceiling\n",
        "\n",
        "    # load labels by loading the first one and then concatenating the rest\n",
        "    y = np.load(f'{path_masks}{get_files(path_masks)[0]}')\n",
        "    for filename in get_files(path_masks)[1:]:\n",
        "        temp = np.load(f'{path_masks}{filename}', allow_pickle=True)\n",
        "        y = np.concatenate((y, temp))\n",
        "\n",
        "    del temp, ceiling, num_imgs\n",
        "\n",
        "    return (X, y)\n",
        "\n",
        "def extract_labels(X, y):\n",
        "    \"\"\"\n",
        "    Labels are sparse, so they are Get all labels (non-zero elements) from a set of images\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: numpy.ndarray\n",
        "    y: numpy.ndarray\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df: pandas.DataFrame\n",
        "    \"\"\"\n",
        "    # extract non-zero value indices from y (= label position) to extract the corresponding X-value\n",
        "    # prepare data to merge it into one data frame,\n",
        "    # which makes it easier to extract the values of the same pixel\n",
        "    X = X.reshape(10, -1)\n",
        "    y = y.reshape(1, -1)\n",
        "    Xy = np.concatenate((X, y), axis=0)\n",
        "    Xy = Xy.transpose()\n",
        "    data = np.empty((0,11))\n",
        "    data = np.concatenate((data, Xy), axis=0)\n",
        "\n",
        "    indices = np.nonzero(data[:,-1])\n",
        "    labeled_data = data[indices]\n",
        "\n",
        "    # create dataframe with features and labels\n",
        "    df = pd.DataFrame(labeled_data)\n",
        "    column_names = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12', 'Label']\n",
        "    df.columns = column_names\n",
        "\n",
        "    return df\n",
        "\n",
        "def upsample_data(df):\n",
        "    \"\"\"\n",
        "    Upsample underrepresented data\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df: pandas.DataFrame\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    features: pandas.DataFrame\n",
        "    labels: pandas.DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # sort data according to tree height asc\n",
        "    dfs = df.sort_values('Label').reset_index(drop=True)\n",
        "    # create empty data frame to fill\n",
        "    dff = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    index_start = 0\n",
        "    for i in range(3, 37, 3):\n",
        "      #count the number of intances that are in one interval for example 0 - 3 or 15 - 18\n",
        "      index_end = index_start + dfs[\"Label\"][(dfs[\"Label\"] > i - 3) & (dfs[\"Label\"] < i)].count()\n",
        "      # take random smaple of the interval\n",
        "      samp = dfs[index_start:index_end].sample(800)\n",
        "      dff = pd.concat((dff, samp))\n",
        "      index_start = index_end\n",
        "\n",
        "    # add the highest values beacuase there are only a few\n",
        "    dff = pd.concat((dff, dfs[index_start:]))\n",
        "    dftr = dff.sample(frac=1).reset_index(drop=True) #shuffel the dataset randomly\n",
        "\n",
        "    # extract features and labels\n",
        "    features = dftr.iloc[:, 0:10]\n",
        "    labels = dftr.iloc[:,10]\n",
        "\n",
        "    # the length of X and y has to be the same\n",
        "    # assert features.shape[0] == labels.shape[0]\n",
        "    return (features, labels)\n",
        "\n",
        "def calculate_ndvi(X, only_ndvi=False):\n",
        "    \"\"\"\n",
        "    Generate a dataset (X_train, X_test, y_train, y_test) based on the location of zip files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: pd.DataFrame\n",
        "    only_ndvi: boolean\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    # Extract the relevant bands for NDVI calculation\n",
        "    b4, b8 = X['B4'], X['B8']\n",
        "    # Calculate NDVI\n",
        "    ndvi = (b8 - b4) / (b8 + b4)\n",
        "\n",
        "    if only_ndvi:\n",
        "        return(ndvi)\n",
        "\n",
        "    # Add NDVI as a new feature to X\n",
        "    features[\"NDVI\"] = ndvi\n",
        "    return features\n",
        "\n",
        "def generate_dataset(path_images, path_masks, only_ndvi=False, with_ndvi=False):\n",
        "    \"\"\"\n",
        "    Generate a dataset (X_train, X_test, y_train, y_test) based on the location of zip files\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    path_images: String\n",
        "    path_masks: String\n",
        "    Path to the train data (default: None)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "    \"\"\"\n",
        "    X, y = extract_data(path_images, path_masks)\n",
        "    df = extract_labels(X, y)\n",
        "    del X, y\n",
        "    features, labels = upsample_data(df)\n",
        "    del df\n",
        "    if with_ndvi:\n",
        "        features = calculate_ndvi(features, only_ndvi)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0, shuffle=True)\n",
        "    del features, labels\n",
        "    return (X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "KhhlAG6etzGI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# unzip data\n",
        "%cd \"drive/MyDrive/Colab Notebooks/data/\"\n",
        "# use -B flag to rename files if there is a file with its name\n",
        "!for f in *.zip; do unzip -B \"$f\"; done\n",
        "%cd ../../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j6QFmDk78E9",
        "outputId": "b3971dd4-28c1-499a-ec7f-7da5f0f15323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data\n",
            "Archive:  images_02.zip\n",
            "  inflating: images/image_000.npy    \n",
            "  inflating: images/image_001.npy    \n",
            "  inflating: images/image_002.npy    \n",
            "  inflating: images/image_003.npy    \n",
            "  inflating: images/image_004.npy    \n",
            "  inflating: images/image_005.npy    \n",
            "  inflating: images/image_006.npy    \n",
            "  inflating: images/image_007.npy    \n",
            "  inflating: images/image_008.npy    \n",
            "  inflating: images/image_009.npy    \n",
            "  inflating: images/image_010.npy    \n",
            "  inflating: images/image_011.npy    \n",
            "  inflating: images/image_012.npy    \n",
            "  inflating: images/image_013.npy    \n",
            "  inflating: images/image_014.npy    \n",
            "  inflating: images/image_015.npy    \n",
            "  inflating: images/image_016.npy    \n",
            "  inflating: images/image_017.npy    \n",
            "  inflating: images/image_018.npy    \n",
            "  inflating: images/image_019.npy    \n",
            "Archive:  images_train.zip\n",
            "  inflating: images/image_000.npy    \n",
            "  inflating: images/image_001.npy    \n",
            "  inflating: images/image_002.npy    \n",
            "  inflating: images/image_003.npy    \n",
            "  inflating: images/image_004.npy    \n",
            "  inflating: images/image_005.npy    \n",
            "  inflating: images/image_006.npy    \n",
            "  inflating: images/image_007.npy    \n",
            "  inflating: images/image_008.npy    \n",
            "  inflating: images/image_009.npy    \n",
            "  inflating: images/image_010.npy    \n",
            "  inflating: images/image_011.npy    \n",
            "  inflating: images/image_012.npy    \n",
            "  inflating: images/image_013.npy    \n",
            "  inflating: images/image_014.npy    \n",
            "  inflating: images/image_015.npy    \n",
            "  inflating: images/image_016.npy    \n",
            "  inflating: images/image_017.npy    \n",
            "  inflating: images/image_018.npy    \n",
            "  inflating: images/image_019.npy    \n",
            "Archive:  masks_02.zip\n",
            "  inflating: masks/mask_000.npy      \n",
            "  inflating: masks/mask_001.npy      \n",
            "  inflating: masks/mask_002.npy      \n",
            "  inflating: masks/mask_003.npy      \n",
            "  inflating: masks/mask_004.npy      \n",
            "  inflating: masks/mask_005.npy      \n",
            "  inflating: masks/mask_006.npy      \n",
            "  inflating: masks/mask_007.npy      \n",
            "  inflating: masks/mask_008.npy      \n",
            "  inflating: masks/mask_009.npy      \n",
            "  inflating: masks/mask_010.npy      \n",
            "  inflating: masks/mask_011.npy      \n",
            "  inflating: masks/mask_012.npy      \n",
            "  inflating: masks/mask_013.npy      \n",
            "  inflating: masks/mask_014.npy      \n",
            "  inflating: masks/mask_015.npy      \n",
            "  inflating: masks/mask_016.npy      \n",
            "  inflating: masks/mask_017.npy      \n",
            "  inflating: masks/mask_018.npy      \n",
            "  inflating: masks/mask_019.npy      \n",
            "Archive:  masks_train.zip\n",
            "  inflating: masks/mask_000.npy      \n",
            "  inflating: masks/mask_001.npy      \n",
            "  inflating: masks/mask_002.npy      \n",
            "  inflating: masks/mask_003.npy      \n",
            "  inflating: masks/mask_004.npy      \n",
            "  inflating: masks/mask_005.npy      \n",
            "  inflating: masks/mask_006.npy      \n",
            "  inflating: masks/mask_007.npy      \n",
            "  inflating: masks/mask_008.npy      \n",
            "  inflating: masks/mask_009.npy      \n",
            "  inflating: masks/mask_010.npy      \n",
            "  inflating: masks/mask_011.npy      \n",
            "  inflating: masks/mask_012.npy      \n",
            "  inflating: masks/mask_013.npy      \n",
            "  inflating: masks/mask_014.npy      \n",
            "  inflating: masks/mask_015.npy      \n",
            "  inflating: masks/mask_016.npy      \n",
            "  inflating: masks/mask_017.npy      \n",
            "  inflating: masks/mask_018.npy      \n",
            "  inflating: masks/mask_019.npy      \n",
            "/content\n",
            "CPU times: user 361 ms, sys: 48.6 ms, total: 409 ms\n",
            "Wall time: 29.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataset(X_train, X_test, y_train, y_test):\n",
        "  X_train.to_pickle(\"X_train.pkl\")\n",
        "  y_train.to_pickle(\"y_train.pkl\")\n",
        "  X_test.to_pickle(\"X_test.pkl\")\n",
        "  y_test.to_pickle(\"y_test.pkl\")\n",
        "  %cd ../../"
      ],
      "metadata": {
        "id": "g-vRVuT5cJyS"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make directories for all types of datasets."
      ],
      "metadata": {
        "id": "2dVgRqAYl1nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd forest_height/data/\n",
        "!mkdir color_channels color_channels_ndvi ndvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uio4sCnJc-LR",
        "outputId": "961d5ca6-c926-41b3-e658-6742eb35a4ce"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/forest_height/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd forest_height/data/"
      ],
      "metadata": {
        "id": "c0_QkHmvlXze",
        "outputId": "0cb75074-7b37-4223-d41e-f3dfe98954e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/forest_height/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only color channels"
      ],
      "metadata": {
        "id": "BvDYnMUOl7XY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks)\n",
        "%cd content/forest_height/data/color_channels\n",
        "save_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "4K6bHgTBlrD-",
        "outputId": "cecae655-0778-4720-dc5d-7bb8f5319397",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/forest_height/data/color_channels\n",
            "/content/forest_height\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Color channels and ndvi value"
      ],
      "metadata": {
        "id": "nR_XLa-JnHwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks, with_ndvi=True)\n",
        "%cd forest_height/data/color_channels_ndvi\n",
        "save_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "kco9-XSVmgGJ",
        "outputId": "808d006e-7e75-43ef-e953-1c0d04284202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/forest_height/data/color_channels_ndvi\n",
            "/content/forest_height\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only ndvi value"
      ],
      "metadata": {
        "id": "QUBnyCuAnJ-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ..\n",
        "X_train, X_test, y_train, y_test = generate_dataset(path_images, path_masks, with_ndvi=True, only_ndvi=True)\n",
        "%cd forest_height/data/ndvi\n",
        "save_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dCo89CmuP4Y",
        "outputId": "4a90d2ad-c883-4d3c-a27e-356bb5ed1083"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/forest_height/data/ndvi\n",
            "/content/forest_height\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove drive connection as it is no longer needed\n",
        "drive.flush_and_unmount()"
      ],
      "metadata": {
        "id": "wJ1a2sqeJhg1"
      },
      "execution_count": 164,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}